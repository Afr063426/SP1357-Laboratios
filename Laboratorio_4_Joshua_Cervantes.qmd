---
title: Laboratorio 4, Tópicos en análisis datos 1
author: Joshua Isaac Cervantes Artavia
date: today
format: 
    pdf:
        tbl-pos: 'H'
        fig-pos: 'H'
        number-sections: true
        colorlinks: true
        fontfamily: libertinus

---
```{r}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(tidyverse) # To manipulate data, and other things
library(ggforce) # To use ggcircle
library(openxlsx) # To manipulate Excel
library(kableExtra) # To make beatiful tables

tryCatch(
    {
        # Directorio donde se ubica el qmd
        directory <- dirname(rstudioapi::getSourceEditorContext()$path)
        setwd(directory) # Establecer el directorio del archivo como la raiz
    },
    error = function(e) {
        message("")
        print("")
    }
)
```

#

```{r}
# We construct a function to make all the things that are wanted in the by the homework
fn_acp_homework <- function(df, scale = TRUE, name_acp) {
    require(tidyverse) # To manipulate data

    require(ggforce) # To make the unit circle

    require(kableExtra) # To make beatiful tables

    # We select the name of columns of individuals
    name_observations <- colnames(df)[1]

    # We delete the first columns that have names
    names_df <- df[, 1]


    # We select the variables
    df <- df[, -1]


    # We get the number of observations
    n <- nrow(df)

    # We get the center of gravity
    g <- apply(df, 2, mean)

    # We estimate the standard deviation
    sd_n <- sqrt((n - 1) / n) * apply(df, 2, sd)

    # We assign a name to the acp
    df <- (df - matrix(rep(g, n), nrow = n, byrow = TRUE)) /
        matrix(rep(sd_n, n), nrow = n, byrow = TRUE)

    # We assign the name to the ACP
    assign(name_acp, prcomp(df), envir = .GlobalEnv)

    # We get the acp to use it
    acp <- get(name_acp)

    # We make the main plain
    df_pc <- as.data.frame(as.matrix(df) %*% as.matrix(acp$rotation))

    # We add the name of the observations
    df_pc <- cbind(names_df, df_pc)

    # We add the name of the first column
    colnames(df_pc)[1] <- name_observations

    # We select the names of the variables
    names_cols <- colnames(df)


    # We print the ggplot that is the main plain
    print(ggplot(, aes(x = df_pc$PC1, y = df_pc$PC2, label = df_pc[, 1])) +
        geom_label(vjust = 1) +
        geom_point() +
        geom_hline(yintercept = 0) +
        geom_vline(xintercept = 0) +
        theme_minimal() +
        # theme(text = element_text(size = 16)) +
        labs(x = "PC1", y = "PC2", caption = paste("Porcentaje de inercia:",
            sum((acp$sdev)[c(1, 2)]^(2)) / sum(acp$sdev^(2)),
            sep = " "
        )))

    # We estimate the correlation of the principal components
    df_correlations <- data.frame("Variable" = names_cols[1], (n - 1) /
        n * cor(df[, names_cols[1]], df_pc[, -1]))

    for (i in colnames(df)[-1]) {
        df_aux <- data.frame("Variable" = i, (n - 1) / n * cor(df[, i], df_pc[, -1]))
        df_correlations <- rbind(df_correlations, df_aux)
    }

    # We show the correlation circle of variables
    print(ggplot(, aes(
        x = df_correlations$PC1, y = df_correlations$PC2,
        label = df_correlations$Variable
    )) +
        geom_label(vjust = 1) +
        geom_point() +
        geom_hline(yintercept = 0) +
        geom_vline(xintercept = 0) +
        geom_circle(aes(x0 = 0, y0 = 0, r = 1)) +
        coord_fixed() +
        theme_minimal() +
        # theme(text = element_text(size = 16)) +
        labs(x = "PC1", y = "PC2", caption = paste("Porcentaje de inercia:",
            sum((acp$sdev)[c(1, 2)]^(2)) / sum(acp$sdev^(2)),
            sep = " "
        )))

    # We save the eigen values
    eigen_values <- data.frame(
        "Valor propio" = 1:ncol(df),
        "Valor" = acp$sdev^(2), "Porcentaje de inercia" = acp$sdev^(2) /
            sum(acp$sdev^(2)) * 100,
        "Porcentaje de inercia acumulado" = cumsum(acp$sdev^(2)) /
            sum(acp$sdev^(2)) * 100
    )

    # We show the table with values
    print(ggplot(, aes(x = 1:length(acp$sdev), y = acp$sdev^(2))) +
        geom_line() +
        geom_point() +
        theme_minimal() +
        # theme(text = element_text(size = 16)) +
        labs(x = "Número", y = "Valor propio"))

    # We estimate the euclidean norm squared
    df_quality_representation <- apply(df, 1, function(x) sum((x)^(2)))

    # We estimate the quality of representation of each projection
    df_quality_representation <- df_pc[, -1]^2 /
        matrix(rep(df_quality_representation, ncol(df)), ncol = ncol(df))

    # We add the name of the columns
    df_quality_representation <- cbind(
        names_df,
        df_quality_representation
    )

    # We estimate the quality of the main plain
    quality_main_plain_id <- df_quality_representation[, 2] + df_quality_representation[, 3]

    df_quality_representation$Calidad.representacion.plano.principal <- quality_main_plain_id

    # We add the name of the column
    colnames(df_quality_representation)[1] <- name_observations

    #We estimate the communalities
    df_comunalities <- df_correlations %>%
        mutate("Comunalidad" = PC1^2 + PC2^2) %>%
        select(Variable, Comunalidad)
    
    #We return the list the list with the tables
    list(
        eigen_values = eigen_values,
        df_pc = df_pc,
        df_correlations = df_correlations,
        df_quality_representation = df_quality_representation,
        df_comunalities = df_comunalities
    )
}


```




## Ejercicio 8 del libro
```{r}
df_notas_quices <- data.frame(
    Estudiante = c(
        "Inés",
        "Jorge",
        "Lina",
        "Franco",
        "Cecilia",
        "Raúl",
        "Eugenia",
        "Juan"
    ),
    quiz1 = c(90, 80, 50, 85, 75, 56, 92, 100),
    quiz2 = c(88, 85, 65, 82, 78, 43, 75, 85),
    quiz3 = c(56, 67, 40, 80, 60, 48, 77, 80),
    quiz4 = c(77, 82, 63, 87, 75, 33, 86, 100)
)

```


```{r}
tablas_notas <- fn_acp_homework(df_notas_quices, name_acp = "acp_notas_quices")
knitr::kable(tablas_notas$eigen_values,
    booktabs = T,
    caption = "Autovalores"
) %>%
    kable_styling(latex_options = "scale_down", font_size = 10)

knitr::kable(tablas_notas$df_pc,
    booktabs = T,
    caption = "Componentes Principales"
) %>%
    kable_styling(latex_options = "scale_down", font_size = 10)

knitr::kable(tablas_notas$df_correlations,
    booktabs = T,
    caption = "Correlaciones"
) %>%
    kable_styling(latex_options = "scale_down", font_size = 10)

knitr::kable(tablas_notas$df_quality_representation,
    booktabs = T,
    caption = "Calidad de proyección"
) %>%
    kable_styling(latex_options = "scale_down", font_size = 10)

knitr::kable(tablas_notas$df_comunalities,
    booktabs = T,
    caption = "Comunalidad"
) %>%
    kable_styling(font_size = 10)


```

De lo anterior se puede destacar que el mayor porcentaje de inercia está explicado por las primeras dos componentes principales, ya que con estas dos se obtiene un porcentaje cercano al 96%. Además, al observar el gráfico de los valores propios se obtiene que el codo se da en la segunda componente principal. Se encuentra que se tiene un efecto talla, según el círculo de correlaciones.


Se procede a usar la función `PCA` del paquete FactoMineR para comparar los resultados obtenidos

```{r}
#We load the package FactoMineR
library(FactoMineR)
library(factoextra)

#We estimate the principal component, it normalized
pca_notas_facto <- PCA(df_notas_quices[-1],)
pca_notas_facto$eig
```

Se puede observar que los porcentaje de inercia son iguales, sin embargo los autovalores varían ligeramente, posible producto de que se emplee el estimador insesgado en el último caso y en la función programada por nosotros lo hace con el estimador no insesgado la varianza.




## Ejercicio 9 del libro
```{r}
df_notas_frances <- read.xlsx("./Ejercicios-Cap3.xlsx", sheet = "9.NotasFrancesas")
```

```{r}
tablas_notas_francesas <- fn_acp_homework(df_notas_frances, name_acp = "acp_notas_quices")
knitr::kable(tablas_notas_francesas$eigen_values,
    booktabs = T,
    caption = "Autovalores"
) %>%
    kable_styling(latex_options = "scale_down", font_size = 10)

knitr::kable(tablas_notas_francesas$df_pc,
    booktabs = T,
    caption = "Componentes Principales"
) %>%
    kable_styling(latex_options = "scale_down", font_size = 10)

knitr::kable(tablas_notas_francesas$df_correlations,
    booktabs = T,
    caption = "Correlaciones"
) %>%
    kable_styling(latex_options = "scale_down", font_size = 10)

knitr::kable(tablas_notas_francesas$df_quality_representation,
    booktabs = T,
    caption = "Calidad de proyección"
) %>%
    kable_styling(latex_options = "scale_down", font_size = 10)

knitr::kable(tablas_notas_francesas$df_comunalities,
    booktabs = T,
    caption = "Comunalidad"
) %>%
    kable_styling(font_size = 10)


```

De lo observado en este caso se puede decir que 84% de la inercia está explicada por las primeras dos componentes principales. Un aspecto a tener en consideración es que se observan lo que parecen dos codos, sin embargo, el primero se encuentra en la primera componente principal. Por lo que se podría seleccionar estas dos componentes principales. Sin embargo, la variabled de educación física no se ve bien representada por estas dos al observar el círculo de correlaciones. La calidad de proyección es buena para la mayoría en la primera componente y en la segunda. Exceto evelyn que tiene una mejor calidad en la tercera componente principal.

## Ejercicio 10 del libro
```{r}
df_amiard <- read.xlsx("./Ejercicios-Cap3.xlsx", sheet = "10.Amiard")
```

```{r}
tablas_amiard <- fn_acp_homework(df_amiard, name_acp = "acp_notas_quices")
knitr::kable(tablas_amiard$eigen_values,
    booktabs = T,
    caption = "Autovalores"
) %>%
    kable_styling(latex_options = "scale_down", font_size = 10)

knitr::kable(tablas_amiard$df_pc,
    booktabs = T,
    caption = "Componentes Principales"
) %>%
    kable_styling(latex_options = "scale_down", font_size = 10)

knitr::kable(tablas_amiard$df_correlations,
    booktabs = T,
    caption = "Correlaciones"
) %>%
    kable_styling(latex_options = "scale_down", font_size = 10)

knitr::kable(tablas_amiard$df_quality_representation,
    booktabs = T,
    caption = "Calidad de proyección"
) %>%
    kable_styling(latex_options = "scale_down", font_size = 10)

knitr::kable(tablas_amiard$df_comunalities,
    booktabs = T,
    caption = "Comunalidad"
) %>%
    kable_styling(font_size = 10)


```

En este caso se encuentra que las dos componentes principales ocupan un 96% de la inercia total. Además, al observar el gráfico de los autovalores se observa que estos tienen un claro codo en la tercera componente principal. En esre caso sería mejor quedarse con las primeras dos. Se encuentra que la calidad de proyección en estad soc componentes principales es buena, superando el 80% en todos los casos.


Para realizar lo solicitado en el libro de proyectar como variable suplementaria 
```{r}
tablas_amiard_without_x_7 <- fn_acp_homework(df_amiard[, -8], name_acp = "acp_notas_quices")
knitr::kable(tablas_amiard_without_x_7$eigen_values,
    booktabs = T,
    caption = "Autovalores"
) %>%
    kable_styling(latex_options = "scale_down", font_size = 10)

knitr::kable(tablas_amiard_without_x_7$df_pc,
    booktabs = T,
    caption = "Componentes Principales"
) %>%
    kable_styling(latex_options = "scale_down", font_size = 10)

knitr::kable(tablas_amiard_without_x_7$df_correlations,
    booktabs = T,
    caption = "Correlaciones"
) %>%
    kable_styling(latex_options = "scale_down", font_size = 10)

knitr::kable(tablas_amiard_without_x_7$df_quality_representation,
    booktabs = T,
    caption = "Calidad de proyección"
) %>%
    kable_styling(latex_options = "scale_down", font_size = 10)

knitr::kable(tablas_amiard_without_x_7$df_comunalities,
    booktabs = T,
    caption = "Comunalidad"
) %>%
    kable_styling(font_size = 10)


```


```{r}
pca_peces <- PCA(df_amiard[, -c(1)], quanti.sup = 7)


ggsave("./pca_peces.svg", fviz_pca_var(pca_peces))
```

![](pca_peces.svg)


Se puede observar que el comportamiento en los dos ACP es similar sin la variable en cuestión. Salvo que la calidad de proyección mejora en las primeras componentes principales y la comunalidad sigue siendo muy similar.


## Ejercicio 11 del libro
```{r}
df_suicidios <- read.xlsx("./Ejercicios-Cap3.xlsx", sheet = "11.Suicidios")
```

```{r}
tablas_suicidios <- fn_acp_homework(df_suicidios, name_acp = "acp_notas_quices")
knitr::kable(tablas_suicidios$eigen_values,
    booktabs = T,
    caption = "Autovalores"
) %>%
    kable_styling(latex_options = "scale_down", font_size = 10)

knitr::kable(tablas_suicidios$df_pc,
    booktabs = T,
    caption = "Componentes Principales"
) %>%
    kable_styling(latex_options = "scale_down", font_size = 10)

knitr::kable(tablas_suicidios$df_correlations,
    booktabs = T,
    caption = "Correlaciones"
) %>%
    kable_styling(latex_options = "scale_down", font_size = 10)

knitr::kable(tablas_suicidios$df_quality_representation,
    booktabs = T,
    caption = "Calidad de proyección"
) %>%
    kable_styling(latex_options = "scale_down", font_size = 10)

knitr::kable(tablas_suicidios$df_comunalities,
    booktabs = T,
    caption = "Comunalidad"
) %>%
    kable_styling(font_size = 10)
```

En este caso se encuentra que el mayor porcentaje de inercia cercano al 90% se alcanza en la tercera componente principal. Sin embargo, el codo se presenta en la segunda componente principal. Por lo que se pueden seleccionar las primeras dos covariables. Además, en cuanto a la calidad de proyección se encuentra que AlemaniaFed, Estados Unidos, Suiza y Dinamarca no tienen la mejor calidad de proyección. Sin embargo, con la tercera componente principal mejoraría la calidad de manera significativa.


Se verifican los valores obtenidos con `dudi.pca`
```{r}
library(ade4)
dudi_pca_suicidios <- dudi.pca(df_suicidios[,-1], scannf = FALSE, scale = TRUE, center = TRUE)

```

Se obtienen valores similares, más no exactamente iguales, posiblemente motivado por la forma en que se estiman estos con los pesos, podemos verlo acontinuación donde se estima la desviación estándar sin el estimador insesgado y coincide con lo empleado por la función `dudi.pca`.

```{r}
# SD using weigth equal to n
apply(df_suicidios[, -1], 2, sd) * sqrt((nrow(df_suicidios) - 1) / nrow(df_suicidios))

# SD used by dudi.pca
dudi_pca_suicidios$norm
```

## Ejercicio 12 del libro
```{r}
df_proteinas <- read.xlsx("./Ejercicios-Cap3.xlsx", sheet = "12.Proteinas")
```

```{r}
tablas_proteinas <- fn_acp_homework(df_proteinas, name_acp = "acp_notas_quices")
knitr::kable(tablas_proteinas$eigen_values,
    booktabs = T,
    caption = "Autovalores"
) %>%
    kable_styling(latex_options = "scale_down", font_size = 10)

knitr::kable(tablas_proteinas$df_pc,
    booktabs = T,
    caption = "Componentes Principales"
) %>%
    kable_styling(latex_options = "scale_down", font_size = 10)

knitr::kable(tablas_proteinas$df_correlations,
    booktabs = T,
    caption = "Correlaciones"
) %>%
    kable_styling(latex_options = "scale_down", font_size = 10)

knitr::kable(tablas_proteinas$df_quality_representation,
    booktabs = T,
    caption = "Calidad de proyección"
) %>%
    kable_styling(latex_options = "scale_down", font_size = 10)

knitr::kable(tablas_proteinas$df_comunalities,
    booktabs = T,
    caption = "Comunalidad"
) %>%
    kable_styling(font_size = 10)
```

En este caso se encuentra que para con la segunda componente principal se obtiene un porcentaje de inercia del 86%, además se encuentra el codo en el segundo autovalor. En cuanto a la calidad de la proyección se ubica Checoslovaquía la presenta la peor calidad de representación, sin embargo, con la tercera componente esto mejora de manera significativa. En cuanto a las covariables al que tiene mayor comunalidad es la de CERE.



Se comparan los autovalores obtenidos por `acp` de `amap`. Donde se puede notar donde los autovalores son similare, más no iguales motivado por la forma en que son escaladas las variables, diferencia con el estimador insesgado.

```{r}
library(amap)
(amap_pca_proteinas <- amap::acp(df_proteinas[, -1]))
(amap_pca_proteinas$sdev)^2
```



## Ejercicio 13 del libro
```{r}
df_importmex <- read.xlsx("./Ejercicios-Cap3.xlsx", sheet = "13.ImportMex")
```

```{r}
tablas_importmex <- fn_acp_homework(df_importmex, name_acp = "acp_notas_quices")
knitr::kable(tablas_importmex$eigen_values,
    booktabs = T,
    caption = "Autovalores"
) %>%
    kable_styling(latex_options = "scale_down", font_size = 10)

knitr::kable(tablas_importmex$df_pc,
    booktabs = T,
    caption = "Componentes Principales"
) %>%
    kable_styling(latex_options = "scale_down", font_size = 10)

knitr::kable(tablas_importmex$df_correlations,
    booktabs = T,
    caption = "Correlaciones"
) %>%
    kable_styling(latex_options = "scale_down", font_size = 10)

knitr::kable(tablas_importmex$df_quality_representation,
    booktabs = T,
    caption = "Calidad de proyección"
) %>%
    kable_styling(latex_options = "scale_down", font_size = 10)

knitr::kable(tablas_importmex$df_comunalities,
    booktabs = T,
    caption = "Comunalidad"
) %>%
    kable_styling(font_size = 10)
```

En este penúltimo intento se encuentra que las mayoría que la el codo se encuentra en la segunda componente principal, además el porcentaje de inercia explicado por las dos primeras componentes es cercano al 88%. Además, se tiene que encuanto a la calidad de proyección es buena en la mayoría de los años exceptuando los casos 1988 y 1987 que muestran una calidad inferior. 

En cuanto a las trayectorías se puede observar que los años consecutivos en algunos casos se encuentran cercano, sin embargo, este no siempre es el caso teniéndose que por ejemplo el 1981 se aleja bastante de 1980, 1983 de 1984 y 1985 de 1986, de tal forma de que se describe un patrón el que cada dos años un año queda en el lado opuesto del plano. Podría considerarse por cada dos años consecutivos que quedan en el mismo lado del plano el año siguiente se contraponga. 

Los países que tienen una mayor comunalidad son Nicaragua y Honduras.

## Ejercicio 14 del libro
```{r}
df_atletas <- read.xlsx("./Ejercicios-Cap3.xlsx", sheet = "14.Atletas")
```

```{r}
tablas_atletas <- fn_acp_homework(df_atletas, name_acp = "acp_notas_quices")
knitr::kable(tablas_atletas$eigen_values,
    booktabs = T,
    caption = "Autovalores"
) %>%
    kable_styling(latex_options = "scale_down", font_size = 10)


knitr::kable(tablas_atletas$df_pc,
    booktabs = T,
    caption = "Componentes Principales"
) %>%
    kable_styling(latex_options = "scale_down", font_size = 10)

knitr::kable(tablas_atletas$df_correlations,
    booktabs = T,
    caption = "Correlaciones"
) %>%
    kable_styling(latex_options = "scale_down", font_size = 10)

knitr::kable(tablas_atletas$df_quality_representation,
    booktabs = T,
    caption = "Calidad de proyección"
) %>%
    kable_styling(latex_options = "scale_down", font_size = 10)

knitr::kable(tablas_atletas$df_comunalities,
    booktabs = T,
    caption = "Comunalidad"
) %>%
    kable_styling(font_size = 10)
```

En este caso se encuentra que la mayoría de la inercia se encuentra en las primeras dos componentes principales alcanzando cerca del 92% de la inercia total. Adicional el codo se presenta en el segundo autovalor. En cuanto a la calidad de representaci'on se encuentra que salvo D, F y M la mayor'ia presentan una calidad aceptable.

# 
a- 
```{r}
princomp_notas_escolares <- princomp(df_notas_frances[,-1], cor = TRUE )
```

Se encuentra que esta función contiene la raíz cuadrada de los autovalores
```{r}
princomp_notas_escolares$sdev
```

Contiene las correlaciones de las variables respecto a las 
```{r}
princomp_notas_escolares$loadings
```

Contiene el centro de gravedad
```{r}
princomp_notas_escolares$center
```

Contiene la desviación estándar
```{r}
princomp_notas_escolares$scale
```


Contiene el número de observaciones
```{r}
princomp_notas_escolares$n.obs
```

Contiene las componentes principales
```{r}
princomp_notas_escolares$scores
```

Contiene la forma en que se empleó la función
```{r}
princomp_notas_escolares$call
```


b-
```{r}
princomp_general_notas_escolares <- princomp(df_notas_frances[, -1])
princomp_notas_escolares
```

Se puede observar que cambian los autovalores principalmente,
además se obtienen las siguientes componentes principales, que son diferentes de las obtenidas al escalar las variables.

```{r}
princomp_general_notas_escolares$scores

```

#

a- 
```{r}
acp_peces <- prcomp(df_amiard[, -1], scale. = TRUE)

C <- acp_peces$x
acp_peces$sdev^2
u <- acp_peces$rotation
```

Se busca recontruir los datos entonces se obtiene

```{r}
X_tilde <- matrix(rep(0, ncol(C) * nrow(C)), nrow = nrow(C))
for (i in 1:ncol(C)) {
    X_tilde <- X_tilde + C[, i] %*% t(u[, i])
}
X_tilde %>%
    kable(booktabs = T, caption = "Datos reconstruídos con ACP centrados") %>%
    kable_styling(latex_options = "scale_down", font_size = 10)
```

Se encuentra que los datos originales centrados tienen lo siguiente
```{r}
df_original_amiard <- (df_amiard[, -1] - matrix(rep(
    acp_peces$center,
    nrow(df_amiard)
), nrow = nrow(df_amiard), byrow = TRUE)) /
    matrix(rep(apply(df_amiard[, -1], 2, sd), nrow(df_amiard)),
        nrow = nrow(df_amiard), byrow = TRUE
    )
df_original_amiard %>%
    kable(booktabs = T, caption = "Datos originales centrados") %>%
    kable_styling(latex_options = "scale_down", font_size = 10)
```

```{r}
autovalores_mayores_1 <- acp_peces$sdev^2
autovalores_mayores_1 <- autovalores_mayores_1[autovalores_mayores_1 >= 1]

C1 <- C[, 1:length(autovalores_mayores_1)]
U1 <- u[, 1:length(autovalores_mayores_1)]

X_1 <- matrix(rep(0, ncol(C) * nrow(C)), nrow = nrow(C))
for (i in 1:ncol(C1)) {
    X_1 <- X_1 + C1[, i] %*% t(U1[, i])
}
X_1 %>%
    kable(
        booktabs = T,
        caption = "Datos centrados reconstruídos con
         componentes principales asociadas a autovalores
          mayores 1"
    ) %>%
    kable_styling(latex_options = "scale_down", font_size = 10)
```

Para comparar el error se estima el erro cuadrático medio

```{r}
MSE_X1 <- mean(as.matrix((df_original_amiard - X_1)^2))

```

Se encuentra que error cuadrático medio de todas las covariables de `r MSE_X1`.

Ahora se realiza lo mismo, pero únicamente con los primero autovalores
```{r}

primeros_autovalores <- acp_peces$sdev[c(1, 2)]^2


C2 <- C[, 1:length(primeros_autovalores)]
U2 <- u[, 1:length(primeros_autovalores)]

X_2 <- matrix(rep(0, ncol(C) * nrow(C)), nrow = nrow(C))
for (i in 1:ncol(C2)) {
    X_2 <- X_2 + C2[, i] %*% t(U2[, i])
}

X_2 %>%
    kable(
        booktabs = T,
        caption = "Datos centrados reconstruídos con dos componentes principales"
    ) %>%
    kable_styling(latex_options = "scale_down", font_size = 10)

MSE_X2 <- mean(as.matrix((df_original_amiard - X_2)^2))
```

El error cuadrático medio es de `r MSE_X2`, por lo que se encuentra que el error es menor con las `r length(autovalores_mayores_1)` componentes principales, como es de esperarse.

#

```{r}
# To apply Gram - Schmidt algorithm
fn_gram_schmidt <- function(matrix_eigenvectors, M) {
    ortogonal_matrix <- diag(0, nrow = nrow(matrix_eigenvectors))
    ortogonal_matrix[, 1] <- matrix_eigenvectors[, 1]
    ortonormed_matrix <- diag(0, nrow = nrow(matrix_eigenvectors))
    ortonormed_matrix[, 1] <- (ortogonal_matrix[, 1]) *
        1 / sqrt(matrix(ortogonal_matrix[, 1], nrow = 1) %*%
            M %*%
            matrix(ortogonal_matrix[, 1], ncol = 1))
    for (i in 2:ncol(matrix_eigenvectors)) {
        v <- matrix_eigenvectors[, i]
        u <- 0

        for (j in 1:(i - 1)) {
            u_j <- ortogonal_matrix[, j]
            u <- u + (matrix(v, nrow = 1) %*% M %*% matrix(u_j, ncol = 1)) /
                (matrix(u_j, nrow = 1) %*% M %*% matrix(u_j, ncol = 1)) * u_j
        }
        ortogonal_matrix[, i] <- v - u
        # print(ortogonal_matrix, nrow = 1)
        ortonormed_matrix[, i] <- (ortogonal_matrix[, i]) *
            1 / sqrt(matrix(ortogonal_matrix[, i], nrow = 1) %*%
                M %*%
                matrix(ortogonal_matrix[, i], ncol = 1))
    }
    return(ortonormed_matrix)
}

g_quices <- apply(df_notas_quices[, -1], 2, mean)
df_centered_quices <- df_notas_quices[, -1] - matrix(rep(
    g_quices,
    nrow(df_notas_quices)
), nrow = nrow(df_notas_quices), byrow = TRUE)

V <- 1 / nrow(df_centered_quices) *
    t(df_centered_quices) %*%
        diag(1, nrow = nrow(df_centered_quices)) %*%
        as.matrix(df_centered_quices)

M <- diag(c(0.1, 0.2, 0.2, 0.5))

VM <- V %*% M


eigen_values <- eigen(VM)

eigen_vectors <- fn_gram_schmidt(eigen_values$vectors, M)

colnames(eigen_vectors) <- c("PC1", "PC2", "PC3", "PC4")
rownames(eigen_vectors) <- colnames(df_centered_quices)
eigen_vectors <- -eigen_vectors
# t(eigen_vectors) %*% M %*% (eigen_vectors) #Check that is ortonormed
eigen_values$values

eigen_vectors


``` 

Ahora se procede a estimar el ACP, pero normado

```{r}
acp_quices <- prcomp(df_notas_quices[, -1], scale. = TRUE)
acp_quices$sdev^2
```

Se encuentra que la diferencia relativa entre los valores obtenidos es

```{r General ACP}
# We make the main plain
df_pc <- as.data.frame(as.matrix(df_centered_quices) %*% M %*% eigen_vectors)
df_pc <- cbind(df_notas_quices[, 1], df_pc)
colnames(df_pc)[1] <- "Nombres"
colnames(df_pc)[c(2, 3, 4)] <- c("PC1", "PC2", "PC3", "PC4")

names_cols <- colnames(df_centered_quices)
n <- nrow(df_centered_quices)

ggplot(, aes(x = df_pc$PC1, y = df_pc$PC2, label = df_pc[, 1])) +
    geom_label(vjust = 1) +
    geom_point() +
    geom_hline(yintercept = 0) +
    geom_vline(xintercept = 0) +
    theme_minimal() +
    # theme(text = element_text(size = 16)) +
    labs(x = "PC1", y = "PC2", caption = paste("Porcentaje de inercia:",
        sum((eigen_values$values)[c(1, 2)]) / sum(eigen_values$values),
        sep = " "
    ))
ggsave("ACP_general.svg")
df_correlations <- data.frame("Variable" = names_cols[1], (n - 1) /
    n * cor(df_centered_quices[, names_cols[1]], df_pc[, -1]))


# We make the correlation plot
for (i in colnames(df_centered_quices)[-1]) {
    df_aux <- data.frame("Variable" = i, (n - 1) /
        n *
        cor(df_centered_quices[, i], df_pc[, -1]))
    df_correlations <- rbind(df_correlations, df_aux)
}

ggplot(, aes(
    x = df_correlations$PC1, y = df_correlations$PC2,
    label = df_correlations$Variable
)) +
    geom_label(vjust = 1) +
    geom_point() +
    geom_hline(yintercept = 0) +
    geom_vline(xintercept = 0) +
    geom_circle(aes(x0 = 0, y0 = 0, r = 1)) +
    coord_fixed() +
    theme_minimal() +
    # theme(text = element_text(size = 16)) +
    labs(x = "PC1", y = "PC2", caption = paste("Porcentaje de inercia:",
        sum((eigen_values$values)[c(1, 2)]) / sum(eigen_values$values),
        sep = " "
    ))
```


```{r Normado ACP}
# We make the main plain
df_centered_quices <- as.matrix(df_centered_quices) / matrix(rep(apply(df_centered_quices, 2, sd), 8), nrow = 8)
df_pc <- as.data.frame(df_centered_quices %*% as.matrix(acp_quices$rotation))
df_pc <- cbind(df_notas_quices[, 1], df_pc)
colnames(df_pc)[1] <- "Nombres"

names_cols <- colnames(df_centered_quices)
n <- nrow(df_centered_quices)

ggplot(, aes(x = df_pc$PC1, y = df_pc$PC2, label = df_pc[, 1])) +
    geom_label(vjust = 1) +
    geom_point() +
    geom_hline(yintercept = 0) +
    geom_vline(xintercept = 0) +
    theme_minimal() +
    # theme(text = element_text(size = 16)) +
    labs(x = "PC1", y = "PC2", caption = paste("Porcentaje de inercia:",
        sum((eigen_values$values)[c(1, 2)]) / sum(eigen_values$values),
        sep = " "
    ))

df_correlations <- data.frame("Variable" = names_cols[1], (n - 1) /
    n * cor(df_centered_quices[, names_cols[1]], df_pc[, -1]))


# We make the correlation plot
for (i in colnames(df_centered_quices)[-1]) {
    df_aux <- data.frame("Variable" = i, (n - 1) / n * cor(df_centered_quices[, i] / matrix(rep(apply(df_centered_quices, 2, sd), 8), nrow = 8), df_pc[, -1]))
    df_correlations <- rbind(df_correlations, df_aux)
}

ggplot(, aes(
    x = df_correlations$PC1, y = df_correlations$PC2,
    label = df_correlations$Variable
)) +
    geom_label(vjust = 1) +
    geom_point() +
    geom_hline(yintercept = 0) +
    geom_vline(xintercept = 0) +
    geom_circle(aes(x0 = 0, y0 = 0, r = 1)) +
    coord_fixed() +
    theme_minimal() +
    # theme(text = element_text(size = 16)) +
    labs(x = "PC1", y = "PC2", caption = paste("Porcentaje de inercia:",
        sum((eigen_values$values)[c(1, 2)]) / sum(eigen_values$values),
        sep = " "
    ))

```

Se puede observar que en este caso se mantiene un comportamiento similar, teniéndose únicamente desplazamiento tanto en el círculo de correlaciones como en el plano principal.